{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "56297cd0229b57c2ab24c9ec50848a7d86f74b08e1a4202a6e24fa79bb3d45be"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lioche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lioche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "{'Sam Browne', 'Fred Hines', \"'Davy Crockett'\", 'Mexican Mauser', 'Maginot Line', 'Yes Gresham', 'Whitneyville Walker', \"Haven's Colt\", \"Jeff It's\", 'Rifle Association', 'Lane Fleming', 'Virginia Manufactory', 'Walker Colt', 'World War', 'Ordinarily Lane', 'Stephen Rand', 'Rappahannock Forge', 'Premix Company', 'Paterson Colts', 'Germany A', 'Pierre Jarrett', \"Lane Fleming's\", 'Karen Lawrence Pierre', 'March Yes', \"Colonel Walker's Texas Rangers\", 'Elisha Collier', 'Cabot Joyner', 'Stephen Gresham', 'Colt Navy Models', 'Walpole Galleries', 'Humphrey Goode', 'Edison Public Power', 'U S North', 'Philip Cabot', 'Scott County', 'N R A', 'Model Colt Dragoons Rand', 'A Hall', 'Whitneyville Walker Colts', 'S North', 'Elmer Umholtz', 'Cheney Navy', 'U S Martials', 'Civil War Rand', 'National Rifle Association', 'Jeff Lane Fleming', 'Oh Lord', 'Adam Trehearne', \"Jeff I'm\", 'Arnold Rivers', 'Colin MacBride', 'My God Jeff Twenty', 'Confederate Leech'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "\n",
    "# ---\n",
    "def get_tag_that_is_chapter(tag): return tag.name == \"h2\" and tag.contents[0].name == \"a\"\n",
    "\n",
    "def tokenize_text(book_text):\n",
    "    TOKEN_PATTERN = r'\\s+'\n",
    "    regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=True)\n",
    "    word_tokens = regex_wt.tokenize(book_text)\n",
    "    return word_tokens\n",
    "# ---\n",
    "\n",
    "# RAW TEXT VERSION\n",
    "\n",
    "book_num = 17866\n",
    "book_link = f\"https://www.gutenberg.org/ebooks/{book_num}.txt.utf-8\"\n",
    "book_request = requests.get(book_link)\n",
    "\n",
    "assert book_request.status_code == 200, f\"Request of the book failed. Status: {book_request.status_code}\"\n",
    "\n",
    "\n",
    "book_chapters = re.split(r'Chapter [0-9]+', book_request.text, flags=re.IGNORECASE)[1:]\n",
    "book_chapters[-1] = book_chapters[-1].split(\"End of Project Gutenberg\")[0]\n",
    "\n",
    "# Warning : indexes start at 1 !\n",
    "book_chapters_paragraphs = dict()\n",
    "tree_by_chapter = dict()\n",
    "\n",
    "for i, chapter in enumerate(book_chapters):\n",
    "    i = i+1 # Align with chapter num\n",
    "    chapter_paragraphs = list()\n",
    "    paragraphs = chapter.split('\\r\\n\\r\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        clean_paragraph = re.sub(r\"[\\\"\\?\\!\\-\\.\\,\\;\\:\\(\\)\\s]+\", ' ', paragraph)\n",
    "        chapter_paragraphs.append(clean_paragraph)\n",
    "\n",
    "    book_chapters_paragraphs[i] = chapter_paragraphs\n",
    "\n",
    "    # Tag and get the proper nouns\n",
    "    tokens = ''.join(chapter_paragraphs)\n",
    "    tokens = tokenize_text(tokens)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Group them if they are binded. Like \"Lionel Lamy\"\n",
    "    groups = groupby(tagged_tokens, key=lambda x: x[1])\n",
    "    names = [[w for w,_ in words] for tag,words in groups if tag==\"NNP\"]\n",
    "    names = [\" \".join(name) for name in names if len(name)>=2] \n",
    "    \n",
    "    tree_by_chapter[i] = set(names)\n",
    "\n",
    "# ---\n",
    "# Tous les paragraphes (liste) d'un chapitre son accessible via :\n",
    "# book_chapters_paragraphs.get(X) où X est le numéro de chapitre\n",
    "# --\n",
    "# Obtenir les noms propres détectés par chapitre :\n",
    "# tree_by_chapter.get(X) où X est le numéro de chapitre\n",
    "# ---\n",
    "\n",
    "print(tree_by_chapter.get(4))\n",
    "\n",
    "# TODO\n",
    "# Il faut maintenant faire une vérification manuelle des personnages.. (voir le mail du prof)\n",
    "# Je vais regarder comment implémenter l'algorithme de Louvain. \n"
   ]
  }
 ]
}