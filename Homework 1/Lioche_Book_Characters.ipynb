{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "56297cd0229b57c2ab24c9ec50848a7d86f74b08e1a4202a6e24fa79bb3d45be"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Sam Browne', 'Fred Hines', \"'Davy Crockett'\", 'Maginot Line', 'Yes Gresham', 'Whitneyville Walker', \"Haven's Colt\", \"Jeff It's\", 'Rifle Association', 'Lane Fleming', 'Virginia Manufactory', \"Colonel Walker's Texas Rangers—you\", 'World War', 'Ordinarily Lane', 'Walker Colt', 'Stephen Rand', 'Rappahannock Forge', 'Premix Company', 'Paterson Colts', 'Germany A', 'Pierre Jarrett', \"Lane Fleming's\", 'Karen Lawrence Pierre', 'March Yes', 'Elisha Collier', 'Cabot Joyner', 'Stephen Gresham', 'Colt Navy Models', 'Walpole Galleries', 'Humphrey Goode', 'Edison Public Power', 'U S North', 'Philip Cabot', 'Scott County', 'N R A', 'Model Colt Dragoons Rand', 'A Hall', 'Whitneyville Walker Colts', 'S North', 'Elmer Umholtz', 'Cheney Navy', 'U S Martials', 'Civil War Rand', 'National Rifle Association', 'Jeff Lane Fleming', 'Oh Lord', 'Adam Trehearne', \"Jeff I'm\", 'Arnold Rivers', 'Colin MacBride', 'My God Jeff Twenty', 'Confederate Leech'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import groupby\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "\n",
    "# ---\n",
    "def get_tag_that_is_chapter(tag): return tag.name == \"h2\" and tag.contents[0].name == \"a\"\n",
    "\n",
    "def tokenize_text(book_text):\n",
    "    TOKEN_PATTERN = r'\\s+'\n",
    "    regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=True)\n",
    "    word_tokens = regex_wt.tokenize(book_text)\n",
    "    return word_tokens\n",
    "# ---\n",
    "\n",
    "# Use HTML instead of raw-txt\n",
    "book_num = 17866\n",
    "book_link = f\"http://www.gutenberg.org/files/{book_num}/{book_num}-h/{book_num}-h.htm\"\n",
    "book_request = requests.get(book_link)\n",
    "\n",
    "assert book_request.status_code == 200, f\"Request of the book failed. Status: {book_request.status_code}\"\n",
    "\n",
    "book_soup = BeautifulSoup(book_request.content, \"html.parser\")\n",
    "book_chapters = book_soup.find_all(get_tag_that_is_chapter)\n",
    "\n",
    "book_chapters_paragraphs = dict()\n",
    "for chapter in book_chapters:\n",
    "    chapter_paragraphs = list()\n",
    "    for element in chapter.next_siblings:\n",
    "        if element.name == \"hr\" or element.name == \"h2\": break\n",
    "        if element.name == \"p\" and element.get_text() is not None: \n",
    "            clean = re.sub(r\"[\\\"\\?\\!\\-\\.\\,\\;\\:\\(\\)\\s]+\", ' ', element.get_text())\n",
    "            chapter_paragraphs.append(clean)\n",
    "    book_chapters_paragraphs[chapter.contents[0]['id']] = chapter_paragraphs\n",
    "\n",
    "# ---\n",
    "# Tous les paragraphes (liste) d'un chapitre son accessible via :\n",
    "# book_chapters_paragraphs.get(\"CHAPTER_#\")\n",
    "\n",
    "tree_by_chapter = dict()\n",
    "\n",
    "for chapter in book_chapters_paragraphs:\n",
    "    tokens = ''.join(book_chapters_paragraphs.get(chapter))\n",
    "    tokens = tokenize_text(tokens)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    groups = groupby(tagged_tokens, key=lambda x: x[1])\n",
    "    names = [[w for w,_ in words] for tag,words in groups if tag==\"NNP\"]\n",
    "    names = [\" \".join(name) for name in names if len(name)>=2] \n",
    "\n",
    "    tree_by_chapter[chapter] = set(names)\n",
    "\n",
    "# ---\n",
    "# Obtenir les noms propres détectés par chapitre :\n",
    "# tree_by_chapter.get(\"CHAPTER_#\") \n",
    "\n",
    "print(tree_by_chapter.get(\"CHAPTER_4\"))\n",
    "\n",
    "# TODO\n",
    "# Il faut maintenant faire une vérification manuelle des personnages.. (voir le mail du prof)\n",
    "# Je vais regarder comment implémenter l'algorithme de Louvain. "
   ]
  }
 ]
}