{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "Hello world\n",
      "['Hello', 'world']\n",
      "['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import string\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "tbl = str.maketrans({key:None for key in string.punctuation})\n",
    "\n",
    "def remove_punctuation(txt:str) -> str:\n",
    "    return txt.translate(tbl)\n",
    "\n",
    "def tokenize(txt:str):\n",
    "    return nltk.word_tokenize(txt)\n",
    "\n",
    "def stem(token):\n",
    "    return [stemmer.stem(w.lower()) for w in token]\n",
    "\n",
    "ex = \"Hello world!\"\n",
    "print(ex)\n",
    "ex = remove_punctuation(ex)\n",
    "print(ex)\n",
    "ex = tokenize(ex)\n",
    "print(ex)\n",
    "ex = stem(ex)\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see\n"
     ]
    }
   ],
   "source": [
    "s = LancasterStemmer()\n",
    "\n",
    "print(s.stem('seeing'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hello dear \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Tom\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n, how are you my friend ?</div></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Hello dear Tom, how are you my friend ?\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.sentiment)\n",
    "\n",
    "displacy.render(doc, style = \"ent\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  ->  -PRON-  ::  True\n",
      "live  ->  live  ::  False\n",
      "in  ->  in  ::  True\n",
      "the  ->  the  ::  True\n",
      "usa  ->  usa  ::  False\n",
      "We  ->  -PRON-  ::  True\n",
      "are  ->  be  ::  True\n",
      "living  ->  live  ::  False\n",
      "in  ->  in  ::  True\n",
      "the  ->  the  ::  True\n",
      "US  ->  US  ::  True\n",
      "GPE\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "t = nlp(\"I live in the usa! We are living in the US\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for token in t:\n",
    "    #print(token.text,\" -> \", lemmatizer.lemmatize(token.text))\n",
    "    if token.lemma_ not in string.punctuation:\n",
    "        print(token.text,\" -> \", token.lemma_, \" :: \", token.is_stop)\n",
    "for ent in t.ents:\n",
    "    print(ent.label_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nlp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "from typing import List, Iterable, Sequence, Any\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "def tokenize(sentence : str, spacy_model, to_merge_entities:Sequence[str]=[\"GPE\", \"LOC\", \"PERSON\"]) -> Doc:\n",
    "    doc = spacy_model(sentence)\n",
    "\n",
    "    # Retrieve entities to merge\n",
    "    ent_to_split = {ent.text:ent.text.split(' ') for ent in doc.ents if ent.label_ in to_merge_entities}\n",
    "\n",
    "    # Set all entities lemma to the merged name\n",
    "    for complete_entity, splitted_entity in ent_to_split.items():\n",
    "        merged_entity = \"_\".join(splitted_entity)\n",
    "        for token in doc:\n",
    "            if token.text in splitted_entity:\n",
    "                token.lemma_ = merged_entity\n",
    "\n",
    "    return doc\n",
    "\n",
    "def filter(tokens:Doc, to_avoid:Sequence[str]) -> List[str]:\n",
    "    returned_tokens = set()\n",
    "    for token in tokens:\n",
    "        if (token.text not in to_avoid) and (token.lemma_ not in to_avoid) and (not token.is_stop):\n",
    "            returned_tokens.add(token.lemma_)\n",
    "    return list(returned_tokens)\n",
    "\n",
    "def sent_preprocess(sentence:str, spacy_model, to_avoid:Sequence[str]) -> List[str]:\n",
    "    pipe = (partial(tokenize, spacy_model=spacy_model),\n",
    "            partial(filter, to_avoid=to_avoid))\n",
    "    x = sentence\n",
    "    for f in pipe:\n",
    "        x = f(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def df_preprocess(df:pd.DataFrame, column:str, inplace=False, spacy_model_name=\"en_core_web_sm\"):\n",
    "    import string\n",
    "    import warnings\n",
    "    from functools import partial\n",
    "    try:\n",
    "        import spacy\n",
    "    except ImportError:\n",
    "        warnings.warn(\"Please install spacy package\", ImportWarning)\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(spacy_model_name)\n",
    "    except OSError as os_error:\n",
    "        import sys\n",
    "        import subprocess\n",
    "        warnings.warn(f\"spacy model {spacy_model_name} was not yet installed. Install it now.\", ResourceWarning)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", spacy_model_name])\n",
    "        df_preprocess(df=df, column=column, inplace=inplace, spacy_model_name=spacy_model_name)\n",
    "\n",
    "    to_avoid = list(string.punctuation+' ')\n",
    "\n",
    "    try: # Speedup preprocess with parallelization\n",
    "        from pandarallel import pandarallel\n",
    "        pandarallel.initialize(progress_bar=False, verbose=0)\n",
    "\n",
    "        preprocessed_column = df[column].parallel_apply(partial(sent_preprocess, spacy_model=nlp, to_avoid=to_avoid))\n",
    "    except ImportError:\n",
    "        preprocessed_column = df[column].apply(partial(sent_preprocess, spacy_model=nlp, to_avoid=to_avoid))\n",
    "\n",
    "\n",
    "    preprocessed_df = df if inplace else df.copy(deep=True)\n",
    "\n",
    "    preprocessed_df[column] = preprocessed_column\n",
    "\n",
    "    return preprocessed_df\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "txt = \"Book written by Dr. Mickael Jackson and Doctor Dylan Bryan in United States\"\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['write', 'Doctor', 'Dr.', 'Dylan_Bryan', 'United_States', 'book', 'Mickael_Jackson']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "to_avoid = list(string.punctuation+' ')\n",
    "doc = sent_preprocess(txt, nlp, to_avoid=to_avoid)\n",
    "print(doc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prout\n"
     ]
    }
   ],
   "source": [
    "print(\"prout\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}