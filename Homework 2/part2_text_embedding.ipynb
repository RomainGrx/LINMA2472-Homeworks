{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Useful stuff\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AttributeDict(dict):\n",
    "    \"\"\"Like dict but with attribute access and setting\"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in our train dataset : 7.084e+05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Directory containing all train and test csv\n",
    "dataset_dir = os.path.join(os.curdir, 'datasets', 'comments')\n",
    "\n",
    "DEFAULT_COLUMNS = [\"sentence\", \"class\", \"class_idx\"]\n",
    "dataset = AttributeDict(\n",
    "    train=pd.DataFrame(columns=DEFAULT_COLUMNS),\n",
    "    test=pd.DataFrame(columns=DEFAULT_COLUMNS)\n",
    ")\n",
    "\n",
    "classes = {\n",
    "        \"Donald Trump\" : \"Donald-Trump-%s.csv\",\n",
    "        \"Joe Biden\" : \"Joe-Biden-%s.csv\"\n",
    "        }\n",
    "\n",
    "\n",
    "for typ in dataset.keys(): # train, test\n",
    "    for class_idx, (class_name, class_path) in enumerate(classes.items()): # Donald Trump, Joe Biden\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, class_path%typ), index_col=0)\n",
    "        df[\"class\"] =  class_name # Get the name (Donald Trump or Joe Biden)\n",
    "        df[\"class_idx\"] = class_idx # 0 -> Donald Trump, 1 -> Joe Biden\n",
    "        df.columns = DEFAULT_COLUMNS # Force columns name\n",
    "        dataset[typ] = dataset[typ].append(df, ignore_index=True) # Append train from Donald and Joe together (the same for test)\n",
    "\n",
    "assert dataset.train.shape[1:] == (3,), \"dataframe does not contain 3 columns named %s\"%DEFAULT_COLUMNS\n",
    "assert dataset.test.shape[1:] == (3,), \"dataframe does not contain 3 columns named %s\"%DEFAULT_COLUMNS\n",
    "\n",
    "print(f'Number of words in our train dataset : {dataset.train.sentence.apply(lambda x:len(x.split(\" \"))).sum():.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Preprocess all sentences\n",
    "---\n",
    "\n",
    "- Retrieve entities in the text using spacy `en_core_web_sm` and merged all words making the entity\n",
    "- Clean the text using custom filters such as : \n",
    "    - lower cases\n",
    "    - strip non alpha numeric values\n",
    "    - strip punctuation \n",
    "    - strip multiple whitespaces\n",
    "    - finally taking the lemma of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Sequence, List\n",
    "from spacy.tokens.doc import Doc\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags       # strip html tags\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_non_alphanum\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatize = WordNetLemmatizer().lemmatize\n",
    "\n",
    "def tokenize(sentence : str, spacy_model, to_merge_entities:Sequence[str]=[\"GPE\", \"LOC\", \"PERSON\"]) -> Doc:\n",
    "    doc = spacy_model(sentence)\n",
    "\n",
    "    # Retrieve entities to merge\n",
    "    ent_to_split = {ent.text:ent.text.split(' ') for ent in doc.ents if ent.label_ in to_merge_entities}\n",
    "\n",
    "    # Set all entities lemma to the merged name\n",
    "    for complete_entity, splitted_entity in ent_to_split.items():\n",
    "        merged_entity = \"_\".join(splitted_entity)\n",
    "        sentence.replace(complete_entity, merged_entity)\n",
    "\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(),\n",
    "                      strip_non_alphanum,\n",
    "                      strip_punctuation,\n",
    "                      #remove_stopwords,\n",
    "                      strip_multiple_whitespaces]\n",
    "    parsed_line = preprocess_string(sentence, CUSTOM_FILTERS)\n",
    "    parsed_line = [lemmatize(x) for x in parsed_line]\n",
    "\n",
    "    return parsed_line\n",
    "\n",
    "def filter_tokens(tokens:Sequence[str], to_avoid:Sequence[str]) -> List[str]:\n",
    "    returned_tokens = set()\n",
    "    for token in tokens:\n",
    "        if len(token)>1 and (token not in to_avoid):\n",
    "            returned_tokens.add(token)\n",
    "    return list(returned_tokens)\n",
    "\n",
    "\n",
    "\n",
    "def sent_preprocess(sentence:str, spacy_model, to_avoid:Sequence[str]) -> List[str]:\n",
    "    pipe = (partial(tokenize, spacy_model=spacy_model),\n",
    "            partial(filter_tokens, to_avoid=to_avoid))\n",
    "    x = sentence\n",
    "    for f in pipe:\n",
    "        x = f(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def df_preprocess(df:pd.DataFrame, column:str, inplace:bool=False, spacy_model_name:str=\"en_core_web_sm\") -> pd.DataFrame:\n",
    "    import spacy\n",
    "    import string\n",
    "    from functools import partial\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(spacy_model_name)\n",
    "    except OSError as os_error:\n",
    "        import sys\n",
    "        import warnings\n",
    "        import subprocess\n",
    "        warnings.warn(f\"spacy model {spacy_model_name} was not yet installed. Install it now.\", ResourceWarning)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", spacy_model_name])\n",
    "        df_preprocess(df=df, column=column, inplace=inplace, spacy_model_name=spacy_model_name)\n",
    "\n",
    "    to_avoid = list(string.punctuation+' ') + ['\\n', '\\t']\n",
    "\n",
    "    try: # Speedup preprocess with parallelization\n",
    "        from pandarallel import pandarallel\n",
    "        pandarallel.initialize(progress_bar=False, verbose=0)\n",
    "        preprocessed_column = df[column].parallel_apply(partial(sent_preprocess, spacy_model=nlp, to_avoid=to_avoid))\n",
    "    except AttributeError: # not parallelized\n",
    "        preprocessed_column = df[column].apply(partial(sent_preprocess, spacy_model=nlp, to_avoid=to_avoid))\n",
    "\n",
    "\n",
    "    preprocessed_df = df if inplace else df.copy(deep=True)\n",
    "\n",
    "    preprocessed_df[\"preprocessed\"] = preprocessed_column\n",
    "\n",
    "    return preprocessed_df\n",
    "\n",
    "word_set = {\n",
    "    \"Donald Trump\" : set(),\n",
    "    \"Joe Biden\" : set()\n",
    "}\n",
    "\n",
    "def __make_docs(df_row):\n",
    "    global word_set\n",
    "    tokens = df_row.preprocessed\n",
    "    for token in tokens:\n",
    "        word_set[df_row[\"class\"]].add(token)\n",
    "    doc = TaggedDocument(words=tokens, tags=tokens)\n",
    "    return doc\n",
    "\n",
    "def df_make_doc(df:pd.DataFrame, inplace:bool=False) -> pd.DataFrame:\n",
    "\n",
    "    preprocessed_column = df.apply(__make_docs, axis=1)\n",
    "\n",
    "    preprocessed_df = df if inplace else df.copy(deep=True)\n",
    "\n",
    "    preprocessed_df[\"doc\"] = preprocessed_column\n",
    "\n",
    "\n",
    "    return preprocessed_df\n",
    "\n",
    "\n",
    "for typ in (\"train\", \"test\"):\n",
    "    df_preprocess(dataset[typ], \"sentence\", inplace=True) # Preprocess strings\n",
    "    df_make_doc(dataset[typ], inplace=True) # Build TaggedDocument from tokenized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       sentence      class class_idx  \\\n",
      "22569  Go Joe, TAKE TRUMP DOWN!  Joe Biden         1   \n",
      "\n",
      "                       preprocessed  \\\n",
      "22569  [joe, trump, go, take, down]   \n",
      "\n",
      "                                                     doc  \n",
      "22569  ([joe, trump, go, take, down], [joe, trump, go...  \n",
      "['2800', 'pompeo', 'binary', 'decimal', 'consultation']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.train.tail(1))\n",
    "\n",
    "print(list(word_set[\"Joe Biden\"])[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(progress_bar=False)\n",
    "#\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#\n",
    "# # Used to know which class (Donald, Joe) each word belongs to\n",
    "# word_set = {\n",
    "#     \"Donald Trump\" : set(),\n",
    "#     \"Joe Biden\" : set()\n",
    "# }\n",
    "#\n",
    "# def clean_text(text):\n",
    "#     text = re.sub(r'\\|\\|\\|', r' ', text)\n",
    "#     text = text.lower()\n",
    "#     return text\n",
    "#\n",
    "# def tokenize_text(text):\n",
    "#     tokens = []\n",
    "#     for sent in nltk.sent_tokenize(text):\n",
    "#         for word in nltk.word_tokenize(sent):\n",
    "#             if len(word) < 2:\n",
    "#                 continue\n",
    "#             tokens.append(word.lower())\n",
    "#     return tokens\n",
    "#\n",
    "# def make_docs(df_row):\n",
    "#     tokens = tokenize_text(df_row.sentence)\n",
    "#     for token in tokens:\n",
    "#         word_set[df_row[\"class\"]].add(token)\n",
    "#     doc = TaggedDocument(words=tokens, tags=tokens)\n",
    "#     return doc\n",
    "#\n",
    "# # Apply clean text on both train and test dataset\n",
    "# for typ in (\"train\", \"test\"):\n",
    "#     dataset[typ].sentence = dataset[typ].sentence.parallel_apply(clean_text)\n",
    "#\n",
    "# # Apply tokenize and Tagged Documents for both train and test dataset\n",
    "# for typ in (\"train\", \"test\"):\n",
    "#     dataset[typ][\"doc\"] = dataset[typ].apply(make_docs, axis=1)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Build the Doc2Vec model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "EMBEDDINGS_DIMS = 100\n",
    "N_WORKERS = cpu_count()\n",
    "MIN_COUNT = 2\n",
    "WINDOW = 2\n",
    "EPOCHS = 30\n",
    "MODEL_NAME = f\"doc2vec-dims_{EMBEDDINGS_DIMS}-min_count_{MIN_COUNT}-window_{WINDOW}-epochs_{EPOCHS}\"\n",
    "\n",
    "doc2vec_model = Doc2Vec(\n",
    "    vector_size=EMBEDDINGS_DIMS,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=N_WORKERS,\n",
    "    epochs=EPOCHS,\n",
    "    compute_loss=True,\n",
    "    )\n",
    "\n",
    "doc2vec_model.build_vocab(dataset.train.doc.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=30.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d95ba13b3ba6422b9ec4619860a1506a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec, DiffMetric, ConvergenceMetric\n",
    "\n",
    "class PlotLogger(CallbackAny2Vec):\n",
    "    def __init__(self, model):\n",
    "        self.p = tqdm(total=model.epochs)\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.p.update(1)\n",
    "        # self.p.set_postfix_str(f\"Loss {model.get_latest_training_loss():.3e}\")\n",
    "\n",
    "doc2vec_model.train(dataset.train.doc.values,\n",
    "                   total_examples=doc2vec_model.corpus_count,\n",
    "                   epochs=doc2vec_model.epochs,\n",
    "                   callbacks=[PlotLogger(doc2vec_model)]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m~/venv/linma2472/lib/python3.8/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 701\u001B[0;31m             \u001B[0m_pickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfname_or_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    702\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"saved %s object\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-a7d71598304f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mdoc2vec_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_temporary_training_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeep_doctags_vectors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeep_inference\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mdoc2vec_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodels_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMODEL_NAME\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m\".model\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venv/linma2472/lib/python3.8/site-packages/gensim/models/base_any2vec.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, fname_or_handle, **kwargs)\u001B[0m\n\u001B[1;32m    618\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    619\u001B[0m         \"\"\"\n\u001B[0;32m--> 620\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mBaseAny2VecModel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname_or_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    621\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    622\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venv/linma2472/lib/python3.8/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001B[0m\n\u001B[1;32m    702\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"saved %s object\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    703\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# `fname_or_handle` does not have write attribute\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 704\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_smart_save\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname_or_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseparately\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep_limit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    705\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    706\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venv/linma2472/lib/python3.8/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36m_smart_save\u001B[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001B[0m\n\u001B[1;32m    556\u001B[0m                                        compress, subname)\n\u001B[1;32m    557\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 558\u001B[0;31m             \u001B[0mpickle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpickle_protocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    559\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    560\u001B[0m             \u001B[0;31m# restore attribs handled specially\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venv/linma2472/lib/python3.8/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36mpickle\u001B[0;34m(obj, fname, protocol)\u001B[0m\n\u001B[1;32m   1376\u001B[0m     \"\"\"\n\u001B[1;32m   1377\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mfout\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# 'b' for binary, needed on Windows\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1378\u001B[0;31m         \u001B[0m_pickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1379\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1380\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "models_path = os.path.join(os.curdir, 'models')\n",
    "if not os.path.exists(models_path):\n",
    "    os.mkdir(models_path)\n",
    "\n",
    "doc2vec_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "doc2vec_model.save(os.path.join(models_path, MODEL_NAME+\".model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plot the Doc2Vec vectors in 2D using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "vec_tsne = TSNE(n_components=2).fit_transform(doc2vec_model.wv.vectors)\n",
    "vocabulary = list(doc2vec_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def get_class_word(word):\n",
    "    n = 0\n",
    "    if word in word_set[\"Donald Trump\"]:\n",
    "        n += 1\n",
    "    if word in word_set[\"Joe Biden\"]:\n",
    "        n += 2\n",
    "    return n\n",
    "\n",
    "def get_color_word(class_word:int, color_dict=(\"#000000\", \"#FF0000\", \"#0000FF\", \"#00FF00\")):\n",
    "#def get_color_word(class_word:int, color_dict=(\"#000000\", \"red\", \"blue\", \"green\")):\n",
    "    return color_dict[class_word]\n",
    "\n",
    "def get_class_name_word(class_word, classes=(\"\", \"Donald Trump\", \"Joe Biden\", \"Biden and Trump\")):\n",
    "    return classes[class_word]\n",
    "\n",
    "with Pool() as pool:\n",
    "    class_word = pool.map(get_class_word, vocabulary)\n",
    "    color_word = pool.map(get_color_word, class_word)\n",
    "    class_name_word = pool.map(get_class_name_word, class_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook, output_file, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.io import curdoc\n",
    "import numpy as np\n",
    "\n",
    "plot_dict = AttributeDict(\n",
    "    x = vec_tsne[:,0],\n",
    "    y = vec_tsne[:,1],\n",
    "    fill_color = np.array(color_word),\n",
    "    legend_label = np.array(class_name_word),\n",
    "    word = np.array(vocabulary),\n",
    "    class_name_word = np.array(class_name_word)\n",
    "\n",
    ")\n",
    "source = ColumnDataSource(plot_dict)\n",
    "\n",
    "TITLE = \"Plot of Doc2Vec trained vectors in 2D using t-SNE\"\n",
    "TOOLS = \"hover,pan,wheel_zoom,box_zoom,tap,reset,save\"\n",
    "p = figure(tools=TOOLS,\n",
    "           title=TITLE,\n",
    "           toolbar_location=\"above\",\n",
    "           plot_width=800,\n",
    "           plot_height=800)\n",
    "p.xaxis.axis_label = \"t-SNE variable 1\"\n",
    "p.yaxis.axis_label = \"t-SNE variable 2\"\n",
    "output_notebook()\n",
    "\n",
    "curdoc().theme = \"light_minimal\"\n",
    "#output_file(\"doc2vec.html\")\n",
    "\n",
    "# Show info per circle (pan)\n",
    "p.hover.tooltips = [\n",
    "    (\"word\", \"@word\"),\n",
    "    (\"class\",\"@class_name_word\")\n",
    "]\n",
    "\n",
    "# display all circles\n",
    "color_dict=(\"#000000\", \"#FF0000\", \"#0000FF\", \"#00FF00\")\n",
    "\n",
    "p.circle(x=\"x\",\n",
    "         y=\"y\",\n",
    "         radius=.4,\n",
    "         selection_color=\"black\",\n",
    "         fill_alpha=.5,\n",
    "         line_alpha=.6,\n",
    "         fill_color=\"fill_color\",\n",
    "         legend_group='class_name_word',\n",
    "         source=source)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy=\"hide\"\n",
    "show(p)"
   ]
  },
  {
   "source": [
    "# BERT model"
   ],
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')\n",
    "\n",
    "encoded_sentences = embedder.encode(dataset.train.sentence.values)"
   ]
  },
  {
   "source": [
    "## Get the t-SNE of the BERT model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_tsne_bert = TSNE(n_components=2).fit_transform(encoded_sentences)\n",
    "vocabulary = dataset.train.sentence.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_dict = AttributeDict(\n",
    "    x = vec_tsne_bert[:,0],\n",
    "    y = vec_tsne_bert[:,1],\n",
    "    sentence = np.array(vocabulary),\n",
    "    class_name_word = np.array(class_name_word)\n",
    "\n",
    ")\n",
    "source = ColumnDataSource(plot_dict)\n",
    "\n",
    "TITLE = \"Plot of BERT trained vectors in 2D using t-SNE\"\n",
    "TOOLS = \"hover,pan,wheel_zoom,box_zoom,tap,reset,save\"\n",
    "p = figure(tools=TOOLS,\n",
    "           title=TITLE,\n",
    "           toolbar_location=\"above\",\n",
    "           plot_width=800,\n",
    "           plot_height=800)\n",
    "p.xaxis.axis_label = \"t-SNE variable 1\"\n",
    "p.yaxis.axis_label = \"t-SNE variable 2\"\n",
    "\n",
    "curdoc().theme = \"light_minimal\"\n",
    "output_notebook()\n",
    "#output_file(\"doc2vec.html\")\n",
    "\n",
    "# Show info per circle (pan)\n",
    "p.hover.tooltips = [\n",
    "    (\"sentence\", \"@sentence\"),\n",
    "]\n",
    "\n",
    "# display all circles\n",
    "color_dict=(\"#000000\", \"#FF0000\", \"#0000FF\", \"#00FF00\")\n",
    "\n",
    "p.circle(x=\"x\",\n",
    "         y=\"y\",\n",
    "         radius=.4,\n",
    "         selection_color=\"black\",\n",
    "         fill_alpha=.5,\n",
    "         line_alpha=.6,\n",
    "         source=source)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy=\"hide\"\n",
    "show(p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict = AttributeDict(\n",
    "    x = vec_tsne_bert[:,0],\n",
    "    y = vec_tsne_bert[:,1],\n",
    "    sentence = np.array(vocabulary),\n",
    "    class_name_word = np.array(class_name_word)\n",
    "\n",
    ")\n",
    "source = ColumnDataSource(plot_dict)\n",
    "\n",
    "TITLE = \"Plot of BERT trained vectors in 2D using t-SNE\"\n",
    "TOOLS = \"hover,pan,wheel_zoom,box_zoom,tap,reset,save\"\n",
    "p = figure(tools=TOOLS,\n",
    "           title=TITLE,\n",
    "           toolbar_location=\"above\",\n",
    "           plot_width=800,\n",
    "           plot_height=800)\n",
    "p.xaxis.axis_label = \"t-SNE variable 1\"\n",
    "p.yaxis.axis_label = \"t-SNE variable 2\"\n",
    "\n",
    "curdoc().theme = \"light_minimal\"\n",
    "output_notebook()\n",
    "#output_file(\"doc2vec.html\")\n",
    "\n",
    "# Show info per circle (pan)\n",
    "p.hover.tooltips = [\n",
    "    (\"sentence\", \"@sentence\"),\n",
    "]\n",
    "\n",
    "# display all circles\n",
    "color_dict=(\"#000000\", \"#FF0000\", \"#0000FF\", \"#00FF00\")\n",
    "\n",
    "p.circle(x=\"x\",\n",
    "         y=\"y\",\n",
    "         radius=.4,\n",
    "         selection_color=\"black\",\n",
    "         fill_alpha=.5,\n",
    "         line_alpha=.6,\n",
    "         source=source)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy=\"hide\"\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 5.02741873e-01  5.42350531e-01 -4.54913348e-01  3.62396926e-01\n  1.03594136e+00  8.52222681e-01  1.23655945e-01 -7.70760417e-01\n  1.26155245e+00 -1.57635972e-01  9.47151780e-02  1.36731720e+00\n  4.60520744e-01  8.51923525e-01  6.29401267e-01  4.63805705e-01\n -3.32578868e-02 -1.93109792e-02 -2.08698824e-01  2.22453013e-01\n  2.05459315e-02  3.56784552e-01  2.52761096e-01  1.25503314e+00\n -4.37452257e-01  2.11342469e-01  4.83110070e-01  5.16365707e-01\n -6.43678725e-01  2.98054487e-01 -1.55116749e+00 -1.73278544e-02\n -2.15552464e-01 -4.89240438e-01 -1.01642676e-01  3.72244865e-01\n  7.41003692e-01 -2.16345596e+00  8.84128571e-01 -5.11813343e-01\n -1.54283655e+00  2.98755020e-01  5.73809087e-01  5.89497626e-01\n  8.25649679e-01 -1.51504651e-01 -1.53219506e-01  2.69360900e-01\n  7.69193649e-01  6.66561484e-01  1.80424303e-01  9.96109009e-01\n -1.37816638e-01  1.13392308e-01 -6.31846726e-01  8.12728465e-01\n -2.81687289e-01 -4.95755881e-01  4.92992193e-01 -2.54587948e-01\n  5.16135812e-01 -3.63849968e-01 -3.31415981e-01  6.05331719e-01\n  6.95449293e-01 -9.85563695e-02  4.33737040e-01 -1.44980857e-02\n -2.44026184e-02 -6.26050591e-01  6.83746338e-01 -7.55862117e-01\n  4.47669476e-01  1.51801882e-02 -1.00208007e-01 -6.83161765e-02\n -2.82239467e-02  6.65636420e-01  1.94904864e-01  5.93132190e-02\n  1.46987841e-01 -4.43229318e-01 -1.36425352e+00 -1.57430708e+00\n -2.04716995e-02 -2.25018811e+00  1.30055398e-01  3.31576526e-01\n  3.01627398e-01 -7.02887595e-01  2.22919643e-01  4.64836806e-01\n -8.35614324e-01 -6.67970300e-01 -7.57205069e-01 -2.92174488e-01\n  1.03761911e+00 -4.97194439e-01  6.79930151e-02  3.37036848e-01\n -4.54916060e-01 -1.10774249e-01  5.27589060e-02 -5.57083368e-01\n -6.63784146e-02 -4.22189832e-01  4.70038056e-01 -8.62532079e-01\n  1.07390180e-01  6.36615336e-01  5.57894409e-01 -5.50633729e-01\n -4.00935978e-01  2.94311583e-01 -9.13818851e-02  6.25542581e-01\n -1.92120001e-01 -3.85324538e-01  1.45685747e-01  7.11307943e-01\n  4.35548842e-01  8.66142750e-01  1.04417777e+00  1.09363806e+00\n -1.28720319e+00 -9.18982804e-01 -3.17381293e-01 -1.38843203e+00\n -4.95382935e-01 -2.96390682e-01 -1.88156962e-02  2.51577646e-01\n  1.47030085e-01 -4.63619679e-01 -1.11745548e+00 -1.41612113e-01\n -3.89023155e-01 -1.17658153e-01 -5.33905029e-01  3.22536826e-01\n  4.11195248e-01  5.89190781e-01  3.84920835e-01  6.26480877e-02\n -4.88136262e-02  4.83035266e-01  1.65238865e-02  1.12076569e+00\n -8.97645891e-01 -2.83968002e-01 -8.53727400e-01  1.24577117e+00\n  1.70529500e-01 -5.98589838e-01 -6.63692117e-01  4.52659816e-01\n  7.49656677e-01  8.18292677e-01 -6.46175623e-01  7.18621731e-01\n  4.71037358e-01  9.27712142e-01  4.72469389e-01  9.03552473e-01\n  1.41582131e-01  5.29607773e-01 -1.13465321e+00  8.99565667e-02\n  7.02711880e-01  2.56511718e-01 -3.63334984e-01 -4.82015073e-01\n -6.01598859e-01 -4.75319922e-01  9.37916517e-01  9.64972153e-02\n  6.99661672e-01  2.43827049e-03  3.59490007e-01 -1.22897863e+00\n -4.02456135e-01  7.93772116e-02  7.76622295e-01 -4.46447313e-01\n  9.69560206e-01 -7.25417972e-01 -1.14285186e-01 -4.19522822e-01\n -1.38961121e-01  6.34238780e-01 -3.66766490e-02 -1.85470283e-01\n  2.14042366e-01 -4.79306966e-01  7.19657242e-01  2.18699172e-01\n  6.87175453e-01 -9.23957750e-02 -2.92730987e-01  1.07122767e+00\n -1.06108057e+00  7.59633541e-01  4.59431857e-01  3.14606875e-01\n  4.10980731e-01  6.99546814e-01  6.39849007e-01 -4.18118030e-01\n  2.16364995e-01  4.16192174e-01  2.78637618e-01  3.80670369e-01\n -5.48115611e-01  4.27604049e-01  1.85624912e-01  3.58758718e-01\n  2.92228699e-01  2.71705911e-02  1.30786717e-01  7.92420864e-01\n  4.12861407e-01  4.78207320e-01  1.04366779e+00  1.21426749e+00\n  1.06417191e+00  2.32745260e-01  1.45382062e-01  6.94393098e-01\n -9.91393268e-01  2.61083394e-01  4.43515152e-01 -1.33627748e+00\n  3.00780356e-01 -6.65421546e-01 -5.20439088e-01  3.67369205e-01\n -3.22488338e-01  4.10824746e-01 -1.10957515e+00 -1.03949249e-01\n -2.25781441e+00  1.13189667e-01  7.02029467e-01 -5.48319519e-01\n  2.19440728e-01 -5.33656299e-01  9.10446465e-01 -6.57645464e-01\n -1.04057515e+00 -1.10615790e+00 -6.07555926e-01  2.23170146e-01\n  1.13957465e+00  9.08640742e-01 -2.60027468e-01 -6.56671286e-01\n -9.85295355e-01  5.70090078e-02 -8.08812439e-01  2.24690363e-01\n  9.07525659e-01 -4.43808943e-01 -7.07631290e-01 -7.98695266e-01\n  1.33045304e+00  6.18769228e-01 -6.19410753e-01 -7.39428997e-01\n -2.57538021e-01 -1.84537917e-01  1.24109223e-01 -9.63768736e-03\n -2.47417733e-01 -6.81255877e-01 -2.95314759e-01  3.92199963e-01\n  9.37593460e-01  1.33450115e+00  1.95885539e-01  1.33403754e+00\n  2.67056316e-01 -4.23126608e-01  4.05256867e-01 -1.26323938e+00\n  5.41946702e-02 -3.53276670e-01 -8.73519599e-01 -1.42587885e-01\n -2.19926536e-01 -8.50784361e-01 -1.12165406e-01  2.86576986e-01\n  9.53803182e-01 -2.29272723e-01 -7.49299601e-02 -9.17859375e-01\n -6.48779333e-01 -1.20727897e+00  5.99148631e-01 -1.20681606e-01\n  3.86420280e-01  2.24763975e-01  1.24420688e-01  2.50741124e-01\n  4.78667706e-01  1.17375501e-01  8.03833783e-01  2.99008757e-01\n -2.70551533e-01 -1.31313694e+00 -2.31542841e-01 -1.84800351e+00\n -4.14705485e-01 -2.77084172e-01 -1.18154609e+00  9.70838964e-02\n -4.73632842e-01  5.99816203e-01 -1.82892501e-01  1.22744679e+00\n  2.92933464e-01  8.08506429e-01  7.46900499e-01  5.16521752e-01\n -5.41646302e-01 -4.03141141e-01 -9.75061715e-01  2.12728903e-01\n  7.70337939e-01 -7.41689980e-01 -1.75443232e-01  2.19600725e+00\n -5.65530285e-02 -1.23232834e-01  9.59883869e-01  6.25675976e-01\n  3.37709635e-01 -1.73035264e-01  1.41131306e+00 -8.88520420e-01\n  2.49671102e-01 -4.77934629e-01 -2.75378376e-01  5.63917384e-02\n  3.11014682e-01 -2.95280904e-01 -6.85576499e-01  1.80122519e+00\n  2.70681053e-01  1.32807720e+00 -5.60930848e-01  1.00390732e+00\n -5.28321266e-01 -4.63944465e-01 -3.89894903e-01  3.37774545e-01\n -8.09562564e-01 -6.97149634e-01  4.27892834e-01  7.46841550e-01\n -1.84621522e-03  7.72632897e-01  3.57310355e-01 -3.51075023e-01\n  4.28009301e-01 -5.98248959e-01 -1.62132752e+00  3.22854072e-01\n  3.02660435e-01  1.65260494e-01  2.74662286e-01 -1.04077363e+00\n  3.83400589e-01  5.46939909e-01  7.83880576e-02 -6.98606849e-01\n -1.04678020e-01  2.45193616e-01  6.98003888e-01 -2.38602102e-01\n -7.10219085e-01 -1.69077528e+00 -3.32381845e-01  7.18877792e-01\n  2.98297882e-01  1.06183267e+00  9.38123345e-01  1.03848886e+00\n -3.30193907e-01 -7.72586018e-02  6.81275427e-01  2.61702955e-01\n  1.23137034e-01  4.11324114e-01  3.65919948e-01  6.56078756e-02\n -9.56546128e-01  2.70326912e-01  6.50254488e-01 -4.58556145e-01\n  5.36661685e-01  2.97599416e-02 -2.07590699e-01 -1.41787434e+00\n  1.09199035e+00  7.01295197e-01  1.56606388e+00 -6.55704290e-02\n  8.14120710e-01 -5.71373105e-01  1.37979910e-01 -4.13377911e-01\n  3.62771004e-01  3.28586608e-01 -1.62632972e-01  4.37406272e-01\n -1.17176145e-01 -2.69129515e-01  7.86946952e-01 -3.57056975e-01\n -2.37045109e-01  3.50089341e-01  6.45915046e-02  1.62838429e-01\n  2.06315011e-01  7.63736427e-01 -7.03348875e-01  1.61816940e-01\n -6.46582782e-01  3.54883790e-01  3.96810561e-01 -4.44726318e-01\n -3.83646786e-01  1.10809255e+00 -4.35510784e-01  9.42768604e-02\n  6.52478874e-01 -3.53335917e-01 -1.78634852e-01  4.15196866e-01\n -5.94054163e-01  2.27318600e-01 -1.01052725e+00 -5.93326986e-01\n -1.76166460e-01  1.14658684e-01 -5.43641448e-01 -3.46616715e-01\n -1.13060927e+00  1.05563688e+00 -2.18846455e-01 -4.30680245e-01\n  2.53255725e-01 -1.97507298e+00  1.29079595e-01  8.58497739e-01\n  4.93972935e-03 -7.22222805e-01 -2.17244893e-01  6.87441766e-01\n  5.84900558e-01  3.57871294e-01  9.84822586e-02  6.90044045e-01\n  2.43672118e-01 -9.71937329e-02  7.16538250e-01 -9.84439075e-01\n  1.18072021e+00  8.04034770e-01  3.49626958e-01 -7.76812673e-01\n  2.40684330e-01  1.41165808e-01 -9.09142643e-02 -3.74930203e-02\n  6.67196572e-01 -2.16519386e-01  1.69363484e-01 -2.87392288e-01\n -5.68056166e-01  5.92321813e-01  1.94787279e-01  1.00977071e-01\n  3.85876484e-02  1.17870579e-02 -6.58695936e-01 -4.83578563e-01\n -8.90616655e-01  4.30300891e-01 -8.53718445e-02 -1.32718548e-01\n  1.28908539e+00 -3.01820874e-01 -1.64575465e-02 -6.97579503e-01\n  3.09936285e-01  1.68151462e+00 -5.36907971e-01 -4.41026628e-01\n  3.03339630e-01  5.13151228e-01  9.23592150e-01  7.25582018e-02\n -2.18649022e-02  2.41480470e-01 -5.76915085e-01 -9.20521915e-01\n  9.25038457e-01  7.91262686e-02 -4.09569778e-03 -2.62764275e-01\n -3.60286564e-01  3.86102527e-01  2.08884925e-01  3.61333132e-01\n -6.35122597e-01  5.72195292e-01 -2.13954955e-01  1.96124747e-01\n -1.26044142e+00  7.99953490e-02  7.50325173e-02  7.78695166e-01\n  6.60880730e-02  1.13302898e+00 -1.36500859e+00 -9.54903588e-02\n -7.99864292e-01 -1.48784184e+00  7.42751360e-01 -4.55274463e-01\n -6.99079096e-01  4.21790443e-02  1.68916214e+00  4.18477565e-01\n -9.92359400e-01 -2.66508043e-01 -8.56555641e-01 -1.25248504e+00\n  2.04945117e-01  4.11622852e-01  1.21102720e-01  1.07066977e+00\n -2.32428622e+00  7.56514668e-01 -6.22500926e-02 -1.43564761e-01\n -1.85900196e-01  7.52063692e-02 -8.41187313e-02 -1.85430741e+00\n  6.64490104e-01  7.81983256e-01  7.06595719e-01  5.33979058e-01\n  4.18303668e-01 -6.34948760e-02  1.24399029e-01  9.57779765e-01\n  9.55381334e-01 -1.54700768e+00  2.11406454e-01 -1.51767939e-01\n -6.59287395e-03 -1.72431737e-01  1.71725586e-01  1.29702282e+00\n -1.00826848e+00 -4.21898924e-02 -7.39014030e-01 -4.54565287e-01\n -9.82860103e-02  7.74886787e-01  1.13125336e+00  6.90997958e-01\n -1.78710073e-01 -1.81612086e+00 -4.09208089e-02  7.05892980e-01\n  1.14196679e-02 -1.46104455e-01 -1.82681113e-01 -8.33981335e-02\n -2.20970184e-01 -7.76635468e-01  7.26842344e-01  5.79596400e-01\n  4.58396636e-02 -4.95913833e-01 -1.93366885e-01 -4.62480187e-01\n  6.24331713e-01 -1.09401166e+00  8.85366917e-01 -1.29854488e+00\n  5.36515236e-01  4.92591888e-01 -5.02734065e-01  1.15926065e-01\n  4.40633267e-01 -2.76398093e-01 -5.36416888e-01 -8.57119024e-01\n -2.32126445e-01  6.57954037e-01  2.18491867e-01 -5.06982028e-01\n  4.16153640e-01  1.30658925e+00  7.33511269e-01 -9.57142174e-01\n  6.37367368e-01  4.82697248e-01  3.09961557e-01  1.41299224e+00\n -1.98100835e-01  7.12657630e-01  3.18828642e-01  8.85524511e-01\n  1.49863636e+00  6.24707639e-01 -1.72938752e+00 -7.04892218e-01\n  1.24978805e+00  1.53244352e-02 -4.93421882e-01  7.54837155e-01\n -2.63712525e-01 -1.28714752e+00  8.31805840e-02 -8.53526652e-01\n -6.27880096e-01 -6.70046434e-02  4.24824953e-01 -7.39811182e-01\n -1.16790438e+00 -3.73082906e-01  9.67491511e-03  2.98619956e-01\n  3.31648022e-01 -3.86848766e-03 -1.06354533e-02 -7.62249455e-02\n  1.88400939e-01  1.39954627e+00  2.26842565e-03  1.00408852e+00\n  8.16451728e-01  1.35411215e+00  8.42349946e-01 -6.43605292e-01\n -9.89843085e-02 -5.45065999e-01  2.38624290e-01  1.77266848e+00\n -3.28352675e-02  4.62615550e-01  7.21098840e-01 -8.19837451e-02\n -8.52763280e-02 -1.05377150e+00  4.55171406e-01 -1.66578501e-01\n -6.52502179e-02  1.19288635e+00 -1.80233389e-01  1.80981553e+00\n -2.93137819e-01  5.50539374e-01 -7.25157976e-01 -9.96784270e-01\n  7.50529766e-03  2.01803837e-02 -9.95210171e-01 -3.08444239e-02\n  8.58981073e-01 -3.73304218e-01 -8.93202960e-01  1.33750689e+00\n  2.79432148e-01  9.00280774e-01 -2.68971711e-01 -1.77894235e-01\n  5.56348801e-01 -8.74190986e-01  1.24425730e-02  6.21385634e-01\n  4.28734541e-01 -1.15051590e-01 -6.43990278e-01 -5.68376720e-01\n -7.79451311e-01 -2.51554638e-01 -5.28488874e-01  3.52210552e-01\n -6.83118999e-02 -1.38484687e-01  3.73517662e-01  6.64806783e-01\n  1.39259720e+00 -3.59412551e-01 -2.92344272e-01 -3.04957718e-01\n -6.13424957e-01 -7.28327870e-01  2.40063816e-02  1.51050389e-01\n -9.64420378e-01 -3.14392298e-01  2.25875124e-01  3.52577657e-01\n -1.22556388e-01 -5.62955558e-01 -1.25715397e-02  1.52355269e-01\n  4.02864426e-01 -8.14239204e-01  2.17554763e-01  1.25330484e+00\n  6.86460137e-01 -5.29231310e-01 -1.19664229e-01  1.59992188e-01\n -1.41111955e-01 -1.66392714e-01  5.74686885e-01 -3.89137864e-01\n  4.81100343e-02  8.57411921e-01 -9.00794208e-01 -9.58450437e-01\n  6.76354706e-01 -1.16541348e-01  8.29795837e-01 -1.05940521e+00\n -8.31027746e-01 -5.99633455e-01 -6.06880903e-01 -5.53218901e-01\n -1.15299916e+00  2.88594097e-01  3.73826802e-01  2.56830659e-02\n -9.87542495e-02  6.75593793e-01 -5.43339491e-01  2.84573108e-01\n -5.13132930e-01  1.45227516e+00  8.41568768e-01 -7.30273187e-01\n -2.88535923e-01  2.23918572e-01  6.51107013e-01  3.83357286e-01\n  8.21649358e-02 -6.72751844e-01 -1.50715613e+00 -3.71113330e-01\n  2.22497463e-01  2.20114499e-01  4.90139782e-01  4.52706479e-02\n -9.07948464e-02  2.39613041e-01  2.11008713e-01 -5.66794634e-01]\n"
     ]
    }
   ],
   "source": [
    "plot_dict = AttributeDict(\n",
    "    x = vec_tsne_bert[:,0],\n",
    "    y = vec_tsne_bert[:,1],\n",
    "    sentence = np.array(vocabulary),\n",
    "    class_name_word = np.array(class_name_word)\n",
    "\n",
    ")\n",
    "source = ColumnDataSource(plot_dict)\n",
    "\n",
    "TITLE = \"Plot of BERT trained vectors in 2D using t-SNE\"\n",
    "TOOLS = \"hover,pan,wheel_zoom,box_zoom,tap,reset,save\"\n",
    "p = figure(tools=TOOLS,\n",
    "           title=TITLE,\n",
    "           toolbar_location=\"above\",\n",
    "           plot_width=800,\n",
    "           plot_height=800)\n",
    "p.xaxis.axis_label = \"t-SNE variable 1\"\n",
    "p.yaxis.axis_label = \"t-SNE variable 2\"\n",
    "\n",
    "curdoc().theme = \"light_minimal\"\n",
    "output_notebook()\n",
    "#output_file(\"doc2vec.html\")\n",
    "\n",
    "# Show info per circle (pan)\n",
    "p.hover.tooltips = [\n",
    "    (\"sentence\", \"@sentence\"),\n",
    "]\n",
    "\n",
    "# display all circles\n",
    "color_dict=(\"#000000\", \"#FF0000\", \"#0000FF\", \"#00FF00\")\n",
    "\n",
    "p.circle(x=\"x\",\n",
    "         y=\"y\",\n",
    "         radius=.4,\n",
    "         selection_color=\"black\",\n",
    "         fill_alpha=.5,\n",
    "         line_alpha=.6,\n",
    "         source=source)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy=\"hide\"\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}